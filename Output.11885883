
Start time: 2024-11-05 04:47:15
Job ID:  11885883
config['sample_size'] : %d 17368
8 available workers for ProcessPoolExecutor.
Grid replication: 0, for config number: 0



xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Replications_M1:   0%|                                                                                                               | 0/4 [00:00<?, ?it/s]
Replication # -------------->>>>>  1
df ==================> :  (36926, 51) Total data points:  18463.0
Probs matrix Minimum of -> Max values over each rows:  0.33493053967709385 

Probs matrix Minimum of -> Max values over each rows:  0.5007142358184075 

number of deletes 1056
P_A2_H2 max, min, avg tensor(1.) tensor(0.1014) tensor(0.9720)
P_A1_H1 max, min, avg tensor(1.) tensor(0.1000) tensor(0.6215)
pi_tensor dimensions:  torch.Size([6, 17407])
shape torch.Size([6, 8703])
dimesnions of input stage 8703
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Training started!

***************************************** Train -> Agent #: 0*****************************************

Total time taken to run surr_opt: 67.70356559753418 seconds
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Evaluation started
df ==================> :  (36926, 51) Total data points:  18463.0
Probs matrix Minimum of -> Max values over each rows:  0.3389881125793169 

Probs matrix Minimum of -> Max values over each rows:  0.5017350340663925 

number of deletes 1054
P_A2_H2 max, min, avg tensor(1.) tensor(0.1007) tensor(0.9753)
P_A1_H1 max, min, avg tensor(1.) tensor(0.1000) tensor(0.6212)
pi_tensor dimensions:  torch.Size([6, 17409])
==========================================================================================
pi_10:  0.6099377870559692 pi_11:  0.16265904903411865 pi_12:  0.2274031788110733
pi_20:  0.35392746329307556 pi_21:  0.2074512094259262 pi_22:  0.4386213421821594
==========================================================================================

==========================================================================================
Y1_beh mean:  tensor(79.8572, dtype=torch.float64)
Y2_beh mean:  tensor(79.9153, dtype=torch.float64)
Y1_beh+Y2_beh mean:  tensor(159.7725, dtype=torch.float64)
==========================================================================================

============================
|  Direct Search's method  |
============================
============================================================

***************************************** Test -> Agent #: 0*****************************************

Replications_M1:   0%|                                                                                                               | 0/4 [01:43<?, ?it/s]
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2461, in run_training_with_params
    return run_training(config, config_fixed, current_config, V_replications, config_number, replication_seed=i)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2446, in run_training
    V_replications, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = simulations(V_replications, local_config, config_fixed, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2425, in simulations
    V_replications, df_DQL, df_DS, df_Tao, param_W_DQL, param_W_DS = eval_DTR(V_replications, replication, df_DQL, df_DS, df_Tao, params_DQL_u, params_DS_u, tmp, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2215, in eval_DTR
    df_DS, V_rep_DS, param_W_DS = evaluate_method_DS('DS', params_ds, config_number, df_DS, test_input_stage1, A1_tensor_test, test_input_stage2,
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1520, in evaluate_method_DS
    A1 = compute_test_outputs(nn=nn_stage1,
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1107, in compute_test_outputs
    test_outputs_i = nn(test_input)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 494, in forward
    outputs.append(network(x))
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_final.py", line 2714, in <module>
    main()
  File "run_final.py", line 2686, in main
    run_grid_search(config, config_fixed, param_grid)
  File "run_final.py", line 2509, in run_grid_search
    performance_DQL, performance_DS, performance_Tao, performance_Beh, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = future.result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
