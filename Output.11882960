
Start time: 2024-11-04 12:49:30
Job ID:  11882960
config['sample_size'] : %d 17368
8 available workers for ProcessPoolExecutor.
Grid replication: 0, for config number: 0



xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Replications_M1:   0%|                                                                                                                                                                                                                  | 0/4 [00:00<?, ?it/s]
Replication # -------------->>>>>  1
df ==================> :  (36926, 51) Total data points:  18463.0
Probs matrix Minimum of -> Max values over each rows:  0.33493053967709385 

Probs matrix Minimum of -> Max values over each rows:  0.5007142358184075 

number of deletes 1056
P_A2_H2 max, min, avg tensor(1.) tensor(0.1014) tensor(0.9720)
P_A1_H1 max, min, avg tensor(1.) tensor(0.1000) tensor(0.6215)
pi_tensor dimensions:  torch.Size([6, 17407])
shape torch.Size([6, 8703])
dimesnions of input stage 8703
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Training started!
Replications_M1:   0%|                                                                                                                                                                                                                  | 0/4 [00:39<?, ?it/s]
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2278, in run_training_with_params
    return run_training(config, config_fixed, current_config, V_replications, config_number, replication_seed=i)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2263, in run_training
    V_replications, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = simulations(V_replications, local_config, config_fixed, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2190, in simulations
    trn_val_loss_tpl_DQL = DQlearning(tuple_train, tuple_val, params_DQL_f, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1891, in DQlearning
    train_losses_stage2, val_losses_stage2, epoch_num_model_2 = train_and_validate(config_number, nn_stage2, optimizer_2, scheduler_2,
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1015, in train_and_validate
    train_loss = process_batches_DQL(model, train_inputs, train_actions, train_targets, params, optimizer, is_train=True)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 982, in process_batches_DQL
    inputs_batch = torch.index_select(inputs, 0, batch_idx).to(device)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_final.py", line 2531, in <module>
    main()
  File "run_final.py", line 2503, in main
    run_grid_search(config, config_fixed, param_grid)
  File "run_final.py", line 2326, in run_grid_search
    performance_DQL, performance_DS, performance_Tao, performance_Beh, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = future.result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
