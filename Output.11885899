
Start time: 2024-11-05 05:52:58
Job ID:  11885899
config['sample_size'] : %d 17368
8 available workers for ProcessPoolExecutor.
Grid replication: 0, for config number: 0



xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Replications_M1:   0%|                                                                                                                             | 0/4 [00:00<?, ?it/s]
Replication # -------------->>>>>  1
df ==================> :  (3000, 51) Total data points:  1500.0
Probs matrix Minimum of -> Max values over each rows:  0.342986139797364 

Probs matrix Minimum of -> Max values over each rows:  0.5043867930320002 

number of deletes 84
P_A2_H2 max, min, avg tensor(1.) tensor(0.1977) tensor(0.9716)
P_A1_H1 max, min, avg tensor(0.9941, device='cuda:0') tensor(0.1016, device='cuda:0') tensor(0.6256, device='cuda:0')
pi_tensor dimensions:  torch.Size([6, 1416])
shape torch.Size([6, 708])
dimensions of input stage 708
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Training started!

***************************************** Train -> Agent #: 0*****************************************

Improved ---> ema_val_loss: -511.5220031738281, best_val_loss: inf. Saving the model...
 total_steps   ---->   3
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [1/30], Average Training Loss: -690.3698, Average Validation Loss: -511.5220031738281
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -488.1057983398438, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
Did not improve ---> ema_val_loss: -457.02977172851564, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
 total_steps   ---->   6
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [2/30], Average Training Loss: -698.2472, Average Validation Loss: -408.99351501464844
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -429.04156469726564, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
 total_steps   ---->   9
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [3/30], Average Training Loss: -853.8509, Average Validation Loss: -363.7357482910156
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -409.30344526367185, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Validation loss stabilized <<<<<<<<<<---------->>>>>>>>>> reinitializing model at epoch 4
Did not improve ---> ema_val_loss: -420.3386506982422, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
 total_steps   ---->   12
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [4/30], Average Training Loss: -485.4335, Average Validation Loss: -404.6676483154297
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -406.3547770463867, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
 total_steps   ---->   15
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [5/30], Average Training Loss: -608.9911, Average Validation Loss: -373.7257385253906
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -395.8578776238769, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
Did not improve ---> ema_val_loss: -385.80645916093255, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Validation loss stabilized <<<<<<<<<<---------->>>>>>>>>> reinitializing model at epoch 6
 total_steps   ---->   18
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [6/30], Average Training Loss: -619.4416, Average Validation Loss: -366.859130859375
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -416.3784252212465, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
 total_steps   ---->   21
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [7/30], Average Training Loss: -771.4637, Average Validation Loss: -487.7130126953125
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -432.8984273179584, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
Did not improve ---> ema_val_loss: -444.3381367934693, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
 total_steps   ---->   24
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [8/30], Average Training Loss: -738.7426, Average Validation Loss: -471.2379455566406
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -452.75066059917845, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Validation loss stabilized <<<<<<<<<<---------->>>>>>>>>> reinitializing model at epoch 9
 total_steps   ---->   27
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [9/30], Average Training Loss: -585.8803, Average Validation Loss: -472.3798828125
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -451.26296852294047, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
Did not improve ---> ema_val_loss: -446.7859486935974, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
 total_steps   ---->   30
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [10/30], Average Training Loss: -577.0093, Average Validation Loss: -442.0656280517578
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -438.40569692243224, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
 total_steps   ---->   33
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [11/30], Average Training Loss: -510.9375, Average Validation Loss: -418.8517761230469
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -433.96467632226506, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Early stopping after 12 epochs due to no further improvement.
 total_steps   ---->   34
 num_batches   ---->   1
 num_val_steps   ---->   1

Epoch [12/30], Average Training Loss: -750.5050, Average Validation Loss: -423.602294921875
__________________________________________________________________________________________

LOSS TABLE: 
Epoch Avg Training Loss Avg Validation Loss
    1       -690.369771         -511.522003
    2       -698.247192         -408.993515
    3       -853.850922         -363.735748
    4       -485.433492         -404.667648
    5       -608.991109         -373.725739
    6       -619.441569         -366.859131
    7       -771.463704         -487.713013
    8       -738.742554         -471.237946
    9         -585.8803         -472.379883
   10       -577.009338         -442.065628
   11        -510.93752         -418.851776
   12       -750.505005         -423.602295

Model stage 1 saved successfully at models/11885899/best_model_stage_surr_1_17368_config_number_0_ensemble_num_0.pt
Model stage 2 saved successfully at models/11885899/best_model_stage_surr_2_17368_config_number_0_ensemble_num_0.pt
Total time taken to run surr_opt: 1.8639049530029297 seconds
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Evaluation started
df ==================> :  (3000, 51) Total data points:  1500.0
Probs matrix Minimum of -> Max values over each rows:  0.3489005106121682 

Probs matrix Minimum of -> Max values over each rows:  0.5288146014711255 

number of deletes 81
P_A2_H2 max, min, avg tensor(1.) tensor(0.1225) tensor(0.9878)
P_A1_H1 max, min, avg tensor(0.9938, device='cuda:0') tensor(0.1004, device='cuda:0') tensor(0.6227, device='cuda:0')
pi_tensor dimensions:  torch.Size([6, 1419])
Replications_M1:   0%|                                                                                                                             | 0/4 [00:06<?, ?it/s]
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2900, in run_training_with_params
    return run_training(config, config_fixed, current_config, V_replications, config_number, replication_seed=i)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2885, in run_training
    V_replications, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = simulations(V_replications, local_config, config_fixed, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2864, in simulations
    V_replications, df_DQL, df_DS, df_Tao, param_W_DQL, param_W_DS = eval_DTR(V_replications, replication, df_DQL, df_DS, df_Tao, params_DQL_u, params_DS_u, tmp, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2554, in eval_DTR
    processed_result = load_and_preprocess_data(params_ds, replication_seed=num_replications+1234, config_seed=config_number, run='test')
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2013, in load_and_preprocess_data
    P_A2_given_H2_tensor_filtered = P_A2_given_H2_tensor_filtered[test_patient_ids].to(device)
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_final.py", line 3153, in <module>
    main()
  File "run_final.py", line 3125, in main
    run_grid_search(config, config_fixed, param_grid)
  File "run_final.py", line 2948, in run_grid_search
    performance_DQL, performance_DS, performance_Tao, performance_Beh, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = future.result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)
