
Start time: 2024-11-05 05:24:58
Job ID:  11885892
config['sample_size'] : %d 17368
8 available workers for ProcessPoolExecutor.
Grid replication: 0, for config number: 0



xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Replications_M1:   0%|                                                                                                                             | 0/4 [00:00<?, ?it/s]
Replication # -------------->>>>>  1
df ==================> :  (3000, 51) Total data points:  1500.0
Probs matrix Minimum of -> Max values over each rows:  0.342986139797364 

Probs matrix Minimum of -> Max values over each rows:  0.5043867930320002 

number of deletes 84
P_A2_H2 max, min, avg tensor(1.) tensor(0.1977) tensor(0.9716)
P_A1_H1 max, min, avg tensor(0.9941) tensor(0.1016) tensor(0.6256)
pi_tensor dimensions:  torch.Size([6, 1416])
shape torch.Size([6, 708])
dimesnions of input stage 708
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Training started!

***************************************** Train -> Agent #: 0*****************************************

Improved ---> ema_val_loss: -511.5220031738281, best_val_loss: inf. Saving the model...
 total_steps   ---->   3
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [1/150], Average Training Loss: -690.3698, Average Validation Loss: -511.5220031738281
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -488.1057983398438, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
Did not improve ---> ema_val_loss: -457.02977172851564, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
 total_steps   ---->   6
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [2/150], Average Training Loss: -698.2472, Average Validation Loss: -408.99351501464844
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -429.04156469726564, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
 total_steps   ---->   9
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [3/150], Average Training Loss: -853.8509, Average Validation Loss: -363.7357482910156
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -409.30344526367185, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Validation loss stabilized <<<<<<<<<<---------->>>>>>>>>> reinitializing model at epoch 4
Did not improve ---> ema_val_loss: -420.3386506982422, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
 total_steps   ---->   12
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [4/150], Average Training Loss: -485.4335, Average Validation Loss: -404.6676483154297
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -406.3547770463867, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
 total_steps   ---->   15
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [5/150], Average Training Loss: -608.9911, Average Validation Loss: -373.7257385253906
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -395.8578776238769, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
Did not improve ---> ema_val_loss: -385.80645916093255, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Validation loss stabilized <<<<<<<<<<---------->>>>>>>>>> reinitializing model at epoch 6
 total_steps   ---->   18
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [6/150], Average Training Loss: -619.4416, Average Validation Loss: -366.859130859375
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -416.3784252212465, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
 total_steps   ---->   21
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [7/150], Average Training Loss: -771.4637, Average Validation Loss: -487.7130126953125
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -432.8984273179584, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
Did not improve ---> ema_val_loss: -444.3381367934693, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
 total_steps   ---->   24
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [8/150], Average Training Loss: -738.7426, Average Validation Loss: -471.2379455566406
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -452.75066059917845, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Validation loss stabilized <<<<<<<<<<---------->>>>>>>>>> reinitializing model at epoch 9
 total_steps   ---->   27
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [9/150], Average Training Loss: -585.8803, Average Validation Loss: -472.3798828125
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -451.26296852294047, best_val_loss: -511.5220031738281, no_improvement_count: 0, stabilization_patience: 4
Did not improve ---> ema_val_loss: -446.7859486935974, best_val_loss: -511.5220031738281, no_improvement_count: 1, stabilization_patience: 4
 total_steps   ---->   30
 num_batches   ---->   3
 num_val_steps   ---->   2

Epoch [10/150], Average Training Loss: -577.0093, Average Validation Loss: -442.0656280517578
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -438.40569692243224, best_val_loss: -511.5220031738281, no_improvement_count: 2, stabilization_patience: 4
 total_steps   ---->   33
 num_batches   ---->   3
 num_val_steps   ---->   1

Epoch [11/150], Average Training Loss: -510.9375, Average Validation Loss: -418.8517761230469
__________________________________________________________________________________________
Did not improve ---> ema_val_loss: -433.96467632226506, best_val_loss: -511.5220031738281, no_improvement_count: 3, stabilization_patience: 4
Early stopping after 12 epochs due to no further improvement.
 total_steps   ---->   34
 num_batches   ---->   1
 num_val_steps   ---->   1

Epoch [12/150], Average Training Loss: -750.5050, Average Validation Loss: -423.602294921875
__________________________________________________________________________________________

LOSS TABLE: 
Epoch Avg Training Loss Avg Validation Loss
    1       -690.369771         -511.522003
    2       -698.247192         -408.993515
    3       -853.850922         -363.735748
    4       -485.433492         -404.667648
    5       -608.991109         -373.725739
    6       -619.441569         -366.859131
    7       -771.463704         -487.713013
    8       -738.742554         -471.237946
    9         -585.8803         -472.379883
   10       -577.009338         -442.065628
   11        -510.93752         -418.851776
   12       -750.505005         -423.602295

Model stage 1 saved successfully at models/11885892/best_model_stage_surr_1_17368_config_number_0_ensemble_num_0.pt
Model stage 2 saved successfully at models/11885892/best_model_stage_surr_2_17368_config_number_0_ensemble_num_0.pt
Total time taken to run surr_opt: 2.19999361038208 seconds
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Evaluation started
df ==================> :  (3000, 51) Total data points:  1500.0
Probs matrix Minimum of -> Max values over each rows:  0.3489005106121682 

Probs matrix Minimum of -> Max values over each rows:  0.5288146014711255 

number of deletes 81
P_A2_H2 max, min, avg tensor(1.) tensor(0.1225) tensor(0.9878)
P_A1_H1 max, min, avg tensor(0.9938) tensor(0.1004) tensor(0.6227)
pi_tensor dimensions:  torch.Size([6, 1419])
==========================================================================================
pi_10:  0.6163265705108643 pi_11:  0.16906726360321045 pi_12:  0.2146061211824417
pi_20:  0.3563985526561737 pi_21:  0.21380971372127533 pi_22:  0.42979171872138977
==========================================================================================

==========================================================================================
Y1_beh mean:  tensor(79.5644, dtype=torch.float64)
Y2_beh mean:  tensor(79.7284, dtype=torch.float64)
Y1_beh+Y2_beh mean:  tensor(159.2928, dtype=torch.float64)
==========================================================================================

============================
|  Direct Search's method  |
============================
============================================================

***************************************** Test -> Agent #: 0*****************************************


Top 20 Ensemble Predictions and Majority Votes for A1 (stacked format):
Sample 1:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 2:
  Ensemble A1 predictions + Voted A1 action: [3, 3]
Sample 3:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 4:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 5:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 6:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 7:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 8:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 9:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 10:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 11:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 12:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 13:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 14:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 15:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 16:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 17:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 18:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 19:
  Ensemble A1 predictions + Voted A1 action: [1, 1]
Sample 20:
  Ensemble A1 predictions + Voted A1 action: [1, 1]

Top 20 Ensemble Predictions and Majority Votes for A2 (stacked format):
Sample 1:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 2:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 3:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 4:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 5:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 6:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 7:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 8:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 9:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 10:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 11:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 12:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 13:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 14:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 15:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 16:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 17:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 18:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 19:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Sample 20:
  Ensemble A2 predictions + Voted A2 action: [1, 1]
Replications_M1:   0%|                                                                                                                             | 0/4 [00:07<?, ?it/s]
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/process.py", line 239, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2749, in run_training_with_params
    return run_training(config, config_fixed, current_config, V_replications, config_number, replication_seed=i)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2734, in run_training
    V_replications, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = simulations(V_replications, local_config, config_fixed, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2713, in simulations
    V_replications, df_DQL, df_DS, df_Tao, param_W_DQL, param_W_DS = eval_DTR(V_replications, replication, df_DQL, df_DS, df_Tao, params_DQL_u, params_DS_u, tmp, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 2503, in eval_DTR
    df_DS, V_rep_DS, param_W_DS = evaluate_method_DS('DS', params_ds, config_number, df_DS, test_input_stage1, A1_tensor_test, test_input_stage2,
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1623, in evaluate_method_DS
    V_replications_M1_pred = calculate_policy_values_W_estimator(train_tensors, param_W, A1, A2, P_A1_g_H1, P_A2_g_H2, config_number)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1496, in calculate_policy_values_W_estimator
    result1 = train_and_evaluate(train_data, val_data, test_data, params, config_number, resNum = 1)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1398, in train_and_evaluate
    train_losses_stage2, val_losses_stage2, epoch_num_model_2 = train_and_validate_W_estimator(config_number, nn_stage2, optimizer_2, scheduler_2,
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1232, in train_and_validate_W_estimator
    train_loss = process_batches_DQL(model, train_inputs, train_actions, train_targets, params, optimizer, is_train=True)
  File "/scratch/user/nchapagain/Research/ApplicationDTR/run_final.py", line 1015, in process_batches_DQL
    actions_batch = torch.index_select(actions, 0, batch_idx).to(device)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_final.py", line 3002, in <module>
    main()
  File "run_final.py", line 2974, in main
    run_grid_search(config, config_fixed, param_grid)
  File "run_final.py", line 2797, in run_grid_search
    performance_DQL, performance_DS, performance_Tao, performance_Beh, df_DQL, df_DS, df_Tao, losses_dict, epoch_num_model_lst, config_dict = future.result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/scratch/user/nchapagain/.conda/envs/in-context-learning/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
